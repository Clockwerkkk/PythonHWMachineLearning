{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DF_Kislotni_39_DJ_Babayan_Ushakov_Poletaev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wL9wsZFW2Fb"
      },
      "source": [
        "<strong><font color=darkblue size=6>HW11, Nov-22</font></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBXYXQooW1UU"
      },
      "source": [
        "1. **Kaggle public LB rank & score**: _ _ _ _ _ _ _ _ _ \n",
        "1. **Kaggle team name**: _ _ _ _ _ _ _ _ _ \n",
        "  1. Use format: `[ProGroup ID]-[Your fancy Kaggle group name]-[Team member names]`. \n",
        "  1. Eg. `DA-Avengers-Lazareva,Iazykova,Ovyan` (see Rules in Kaggle for ProGroup ID)\n",
        "1. Our Colab uses [CPU | GPU | TPU]\n",
        "\n",
        "Your private LB score must be reproducible with this Colab. Seed all [RNG](https://en.wikipedia.org/wiki/Random_number_generation). Don't exceed runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8fgHymwXFkQ"
      },
      "source": [
        "<hr color=darkblue>\n",
        "\n",
        "<strong><font color=darkblue size=6>Task 1: Theory (DSBA/ICEF only)</font></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7JMHUlyQ-yP"
      },
      "source": [
        "## **Task 1.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1TRMidDY_Tp"
      },
      "source": [
        "Consider $K$ classes of normally distributed real-valued random variables with shared standard deviation, $\\sigma$, but class-specific means, $\\mu_k$. Show that if class $k$ maximizes the discriminant function, $\\delta_k$, it then also maximizes the posterior for class $k$, i.e. $p_k$ defined in ISLR4.4.1. Rigorously justify each transition. FYI: this is a one-directional claim (mostly algebraic manipulations). *Hint*: carefully review section ISLR4.4 and the relation between the posterior and the corresponding discriminant function. Start by formulaically defining all components, including the posterior and discriminant function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrMKoLDSO5g0"
      },
      "source": [
        "## **SOLUTION**\n",
        "\n",
        "*Your solution goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNy1mpSBRCzp"
      },
      "source": [
        "## **Task 1.2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Mefo4AQ2g7"
      },
      "source": [
        "Given  [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) random variables $X_1,...,X_n$, each of the form $X\\sim\\text{Gamma}(2, \\theta)$ with [PDF](https://en.wikipedia.org/wiki/Probability_density_function):\n",
        "\n",
        "$$f\\left(X \\mid \\theta\\right)=\\theta^{2} \\cdot X \\cdot e ^{-X \\theta}, $$\n",
        "\n",
        "where $ \\quad X>0 , \\quad \\theta>0, \\quad \\mathbb E\\left(X\\right)= \\frac{2}{\\theta}$\n",
        "\n",
        "**(a)** Assign the $\\text{Gamma}(\\alpha, \\beta)$ prior to $\\theta$ and derive an expression of the corresponding [posterior PDF](https://en.wikipedia.org/wiki/Posterior_probability).\n",
        "\n",
        "**(b)** Find a [Bayes estimator](https://en.wikipedia.org/wiki/Bayes_estimator) for $\\theta$ based on the posterior in part (a).\n",
        "\n",
        "**(c)** Discuss how you can calculate a 95% [credible interval](https://en.wikipedia.org/wiki/Credible_interval). You do not need to calculate it. Provide the interpretation of such an interval. \n",
        "\n",
        "**(d)** Assume that $n = 5$ and $\\sum_{i=1}^{n} X_{i}=29.6$. Find the [Bayes factor](https://en.wikipedia.org/wiki/Bayes_factor) for the hypotheses $H_0: \\theta = 0.5$ and $H_1: \\theta = 1$ and provide an interpretation of it. Note that you can answer in terms of logarithms or exponentials and there is no need to give the final number.\n",
        "\n",
        "**Remark**: you can refresh Bayesian statistics by reviewing [*Computer Age Statistical Inference, Ch.3, p.22*](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf#page=40) or [Stanford's STAT200 Lecture Notes](https://web.stanford.edu/class/stats200/Lecture20.pdf). Recommended videos: [üé¶](https://www.youtube.com/results?search_query=bayesian+inference), [&#127910;](https://www.youtube.com/results?search_query=Bayes+estimator), [&#127910;](https://www.youtube.com/results?search_query=credible+interval), [&#127910;](https://www.youtube.com/results?search_query=Bayes+factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWqv112-X8cC"
      },
      "source": [
        "## **SOLUTION**\n",
        "\n",
        "**a)** Since here we have  $\\displaystyle\\text{Gamma}{\\left(\\alpha,\\beta\\right)}$ distribution for each variable, we could define prior density as:\n",
        "\n",
        "$\\displaystyle\\text{Prior density}= f{{\\left({X}\\right)}}=\\frac{{\\beta^{\\alpha}}}{{\\Gamma{\\left(\\alpha\\right)}}}\\cdot{X}^{{\\alpha-{1}}}\\cdot{e}^{{-\\beta\\cdot{X}}}$\n",
        "\n",
        "Next step is likelihood function, which could be defined as follows:\n",
        "\n",
        "$\\displaystyle\\text{Likelihood}= f{{\\left({X}{|}\\theta\\right)}}={\\Pi_{{{i}={1}}}^{{n}}}\\theta^{2}\\cdot{X}_{{i}}\\cdot{e}^{{-{X}_{{i}}\\theta}}$\n",
        "\n",
        "As we know from Bayesian statistics, the posterior density is defined as:\n",
        "\n",
        "$\\displaystyle\\text{Posterior density}\\propto\\text{Likelihood}\\cdot\\text{Prior density}$\n",
        "\n",
        "Next step is to use the log-likelihood function (by taking the log of all expression), since it is more convenient and easy to compute, and we were using this version most part of the course, also, we will omit constants that are non-related to X, since they are proportionalty constants, and we have no need it while computing the posterior density.\n",
        "\n",
        "$\\displaystyle f{{\\left(\\theta{|}{X}\\right)}}\\propto f{{\\left({X}{|}\\theta\\right)}}\\cdot f{{\\left({X}\\right)}}$\n",
        "\n",
        "$\\displaystyle \\log{{\\left( f{{\\left(\\theta{|}{X}\\right)}}\\right)}}\\propto \\log{{\\left( f{{\\left({X}{|}\\theta\\right)}}\\cdot f{{\\left({X}\\right)}}\\right)}}\\propto \\log{{\\left( f{{\\left({X}{|}\\theta\\right)}}\\right)}}+ \\log{{\\left( f{{\\left({X}\\right)}}\\right)}}\\propto \\log{{\\left({\\Pi_{{{i}={1}}}^{{n}}}{X}_{{i}}\\cdot{e}^{{-{X}_{{i}}\\theta}}\\right)}}+ \\log{{\\left({X}^{{\\alpha-{1}}}\\cdot{e}^{{-\\beta{X}}}\\right)}}\\propto{\\Sigma_{{{i}={1}}}^{{n}}} \\log{{\\left({X}_{{i}}\\right)}}-{\\Sigma_{{{i}={1}}}^{{n}}}{X}_{{i}}\\theta+{\\left(\\alpha-{1}\\right)}\\cdot \\log{{\\left({X}\\right)}}-\\beta{X}\\propto\\alpha \\log{{\\left({X}\\right)}}-{X}\\theta-\\beta{X}\\propto\\alpha \\log{{\\left({X}\\right)}}-{\\left(\\theta+\\beta\\right)}{X}$\n",
        "\n",
        "$\\displaystyle f{{\\left(\\theta{|}{X}\\right)}}\\propto{X}^{\\alpha}\\cdot{e}^{{-{\\left(\\alpha+\\beta\\right)}{X}}}$\n",
        "\n",
        "Thus, we can make a conclusion that our posterior denistiy ~ $\\displaystyle\\text{Gamma}{\\left(\\alpha+{1},\\theta+\\beta\\right)}$\n",
        "\n",
        "**b)** As we know, Bayes classifier is defined as:\n",
        "$\\displaystyle{E}{\\left[\\theta{|}{X}\\right]}$\n",
        "\n",
        "Thus, we will compute our Bayes classifier, since Bayes classifier in our case is just a mean value of a posterior distribution:\n",
        "\n",
        "$\\displaystyle\\hat{\\theta}={E}{\\left[\\theta{|}{X}\\right]}=\\frac{{\\alpha+{1}}}{{\\theta+\\beta}}$\n",
        "\n",
        "**—Å)** A 95% credible interval is such an interval that posterior probability of points (frequently here used as a part of estmates such Bayesian one we computed previously) that fall in such an interval is equal to 0.95. In other words, we colud see it as an interval with the boundaries consisting of 0.025 and 0.975 quantiles of posterior density, $\\displaystyle\\text{Gamma}{\\left(\\alpha+{1},\\theta+\\beta\\right)}$, in our case.\n",
        "\n",
        "**d)** Now, with all conditions satisfied, we will try to compute the Bayes classifier, defined as follows:\n",
        "\n",
        "$\\displaystyle{K}=\\frac{{{P}{r}{\\left({X}{|}\\theta={0.5}\\right)}}}{{{P}{r}{\\left({X}{|}\\theta={1}\\right)}}}$\n",
        "\n",
        "Thus, now the calculation part\n",
        "\n",
        "$\\displaystyle{K}=\\frac{{{P}{r}{\\left({X}{|}\\theta={0.5}\\right)}}}{{{P}{r}{\\left({X}{|}\\theta={1}\\right)}}}=\\frac{{{0.5}^{2}\\cdot{X}\\cdot{e}^{{-{0.5}{X}}}}}{{{1}^{2}\\cdot{X}\\cdot{e}^{{-{1}{X}}}}}=\\frac{{{0.25}\\cdot{29.6}\\cdot{e}^{{-{14.8}}}}}{{{1}\\cdot{29.6}\\cdot{e}^{{-{29.6}}}}}=\\frac{{{0.25}\\cdot{3.7362}{e}-{7}}}{{{1.3959}{e}-{13}}}\\approx{669111.2637}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kNhWcb3ZC2O"
      },
      "source": [
        "<hr color=darkblue>\n",
        "\n",
        "<strong><font color=darkblue size=6>Task 2: Kaggle-HAR (for DSBA/ICEF/OOC)</font></strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ8wCtt3audM"
      },
      "source": [
        "\n",
        "**Private URL** for students (allows submission) is in Moodle's HW assignment. **Public URL** with read-only access is [here](https://www.kaggle.com/c/hse-ml-hw11-nov-22-har/rules). See competition rules, submission, grading, dataset, and performance metric. The **starter code** below produces a baseline model, which you should beat, while respecting the competition rules. Your code starts after the timer. This is your baseline model. Remember to seed [RNG](https://en.wikipedia.org/wiki/Random_number_generation) in all experiments for reproducibility.\n",
        "\n",
        "**Instructions for enabling Kaggle API in Colab**:\n",
        "1. Accept competition rules before running [Kaggle API](https://github.com/Kaggle/kaggle-api#api-credentials). [Loading Kaggle dataset example](https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab)\n",
        "1. In your Kaggle Account, [Create API Token](https://github.com/Kaggle/kaggle-api#api-credentials) and save the resulting **kaggle.json** file to the [root of your Google Drive](https://drive.google.com/drive/u/0/my-drive) \n",
        "2. In Colab, open **Files** panel üóÄ (on the left) and click gray folder icon <font color=gray>üñø</font> to mount your Google drive\n",
        "\n",
        "Your Kaggle/Google Drive credentials are secure; and Colab's kaggle.json only lasts a Colab session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A5ccIwGNPgn",
        "outputId": "9aa152cb-769f-466c-883d-926a6ef133ad"
      },
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v hse-ml-hw11-nov-22-har   # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show                       # print public leaderboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: hse-ml-hw11-nov-22-har\n",
            "Using competition: hse-ml-hw11-nov-22-har\n",
            " teamId  teamName                                     submissionDate       score    \n",
            "-------  -------------------------------------------  -------------------  -------  \n",
            "7806289  IB-My Little Pony                            2021-12-01 21:15:22  0.96673  \n",
            "7822157  Alexander Stepin                             2021-12-03 16:35:45  0.96401  \n",
            "7816401  Emil Akopyan                                 2021-12-02 14:26:12  0.96198  \n",
            "7824699  Alexandr Egorov                              2021-12-03 16:48:13  0.96062  \n",
            "7815072  IvanushkinaEkaterina                         2021-12-01 21:38:18  0.95858  \n",
            "7807185  Jingtao Xu                                   2021-12-02 19:27:58  0.95858  \n",
            "7816048  Ushakov Stanislav                            2021-12-03 12:08:59  0.95790  \n",
            "7804803  Rutsinskiy Arsenty                           2021-11-28 23:00:48  0.95723  \n",
            "7809287  Dmitry Ignatev                               2021-12-03 02:00:46  0.94840  \n",
            "7825026  Igor Poletaev                                2021-12-03 14:23:31  0.94704  \n",
            "7815955  Maria Adshead                                2021-11-30 19:45:45  0.94568  \n",
            "7816149  –ë–∞–º–∏–Ω–∏–≤–∞—Ç—Ç–µ –ê—Ä–∞—á—á–∏—Ä–∞–ª–∞–ª–∞–≥–µ –†–∞–Ω–≥–∞ –ù–∞—è–Ω–∞–∫–∞–Ω—Ç–∞  2021-11-30 22:43:21  0.94433  \n",
            "7827237  Steve George Parakal                         2021-12-03 09:25:19  0.93414  \n",
            "7767479  HAR_baseline.csv                             2021-11-22 05:48:49  0.92939  \n",
            "7831170  Arhiburya                                    2021-12-03 15:50:28  0.14460  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yRDzbPONXQ0",
        "outputId": "2c7e5f8b-1a4a-46a7-ff19-c61ace9f2724"
      },
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "!pip -q install -U plotly > log\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\" \n",
        "import numpy as np, pandas as pd, time, matplotlib.pyplot as plt, os, plotly.express as px\n",
        "import tensorflow as tf, tensorflow.keras as keras\n",
        "from sklearn.neural_network import MLPClassifier   # SKLearn's MLP is optimised for CPU (and doesn't use GPU)\n",
        "from keras.layers import Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.set_printoptions(linewidth=10000, precision=2, edgeitems=20, suppress=True)\n",
        "pd.set_option('max_colwidth', 1000, 'max_columns', 100, 'display.width', 1000, 'max_rows', 4)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.09 s, sys: 539 ms, total: 3.63 s\n",
            "Wall time: 7.18 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "CB84KHwAVfRF",
        "outputId": "c81df7bd-50f9-4597-991e-fe4861c5c098"
      },
      "source": [
        "%time vX  = pd.read_csv('testX.csv', index_col='id')  # load testing input features X (only)\n",
        "%time tYX = pd.read_csv('trainYX.csv')                # partially load training labels Y and input features X\n",
        "tYX  # 561 input features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 333 ms, sys: 44 ms, total: 377 ms\n",
            "Wall time: 378 ms\n",
            "CPU times: user 49.9 s, sys: 8.67 s, total: 58.6 s\n",
            "Wall time: 58.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows √ó 562 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54mBGH6zBmY_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qODON1wWBm2j",
        "outputId": "451d1d12-8093-42a2-e0f6-20647cd58cf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATi5O9CIVek3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ac0b338b-e4c6-406d-d2cd-fa5627b1d374"
      },
      "source": [
        "tYX.y.value_counts(sort=False).to_frame().T  # counts of observations in each label category"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>83502</td>\n",
              "      <td>72554</td>\n",
              "      <td>66901</td>\n",
              "      <td>87427</td>\n",
              "      <td>93667</td>\n",
              "      <td>95949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       1      2      3      4      5      6\n",
              "y  83502  72554  66901  87427  93667  95949"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olesMyd1iF2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787ca9ce-3467-46a8-d1c0-a76252f7769b"
      },
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ started. You have 60 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_166qwXkhdN"
      },
      "source": [
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer Start Here...</font></strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snU9iOkJV79w"
      },
      "source": [
        "**TODO. Explain your preprocessing:** i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egL202TMWGbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9379df4d-d09a-415d-b8f9-da0f4a8a9492"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(tYX.drop('y', axis=1).head(50000))\n",
        "tX = pca.transform(tYX.drop('y', axis=1).head(50000))\n",
        "tY = tYX.head(50000).y-1\n",
        "vX = pca.transform(vX)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2j4QCHJn96f"
      },
      "source": [
        "**TODO. Explain your modeling approach:** ideas you tried and why you thought they would be helpful. Takeaway: how these decisions guided you in modeling.\n",
        "\n",
        "**How we did it**\n",
        "---\n",
        "Basically, we started to test different activations, combinations of them and various number of layers. As the practice has shown, activations as ***tanh*** and **exponential** work well on validation split over 0.35, but do not give enough accuracy and have relatively high validation loss. We decided to test **elu**,  **relu** and **selu**. From our practice we found, that **selu** activation performs best with any number of layers as well as on one layer. In this case, we started to test whether we should increase or decrease the number of layers. We found that units for our layers should be proportional, moreover, the larger is the first layer the better accuracy we get on each epoch. Because we knew that more than 2 layers will lead us to overplotting and, as a consequence, lower testing result, we have left two layers of **selu** activation. The next step was to find the proper split of our training set. All **elu** functions(**elu**, **selu**, **relu**) perform better in interval **[0.15,0.2]**, so we took the middle value **0.175**. On each epoch the model trains, but heads right to overplotting. To avoid it and have good results at the same time, we made 2 proportional layers of **selu** activation and gave 6 epochs - this way our model performs well on each stage, takes enough values to reduce loss and produce high accuracy without further overplotting. \n",
        "\n",
        "The best results we got you can see below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a09r0FFzPTM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f71be73-8aad-441d-f3d6-c0ad3fc8003e"
      },
      "source": [
        "tf.random.set_seed(0)   # always seed your experiments\n",
        "Init = keras.initializers.RandomNormal(seed=0) # seed weights and biases\n",
        "\n",
        "m = keras.models.Sequential([\n",
        "    Dense(384, kernel_initializer=Init, input_shape=[tX.shape[1]]),\n",
        "    Dense(96,  kernel_initializer=Init, activation='selu'),\n",
        "    Dense(24,  kernel_initializer=Init, activation='selu'),\n",
        "    Dense(6,  kernel_initializer=Init, activation='softmax')])\n",
        "m.summary()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()              # Maps 0-based integer labels to one-hot encodings\n",
        "m.compile(loss=loss, optimizer=\"sgd\", metrics=[\"accuracy\"])         # Accuracy is not really needed since it's equivalent to loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 384)               215808    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 96)                36960     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 24)                2328      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 150       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 255,246\n",
            "Trainable params: 255,246\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iflSI9xvfrWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead4378f-907c-45e3-af31-68e9280d98df"
      },
      "source": [
        "hist = m.fit(tX, tY, epochs=6, validation_split=.175)  # validation loss is decreasing (as it should)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "1290/1290 [==============================] - 8s 5ms/step - loss: 0.5450 - accuracy: 0.8148 - val_loss: 0.1908 - val_accuracy: 0.9322\n",
            "Epoch 2/6\n",
            "1290/1290 [==============================] - 5s 4ms/step - loss: 0.1172 - accuracy: 0.9632 - val_loss: 0.0803 - val_accuracy: 0.9769\n",
            "Epoch 3/6\n",
            "1290/1290 [==============================] - 5s 4ms/step - loss: 0.0622 - accuracy: 0.9811 - val_loss: 0.0511 - val_accuracy: 0.9857\n",
            "Epoch 4/6\n",
            "1290/1290 [==============================] - 5s 4ms/step - loss: 0.0459 - accuracy: 0.9856 - val_loss: 0.0471 - val_accuracy: 0.9832\n",
            "Epoch 5/6\n",
            "1290/1290 [==============================] - 6s 4ms/step - loss: 0.0383 - accuracy: 0.9888 - val_loss: 0.0338 - val_accuracy: 0.9897\n",
            "Epoch 6/6\n",
            "1290/1290 [==============================] - 6s 4ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.0284 - val_accuracy: 0.9921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoAvjRciDjkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "153d6e27-5c68-455a-de73-e25686f5973c"
      },
      "source": [
        "print('History object contains: ' + ', '.join(hist.history.keys()))\n",
        "dfHist = pd.DataFrame(hist.history)\n",
        "dfHist['epoch'] = dfHist.index+1\n",
        "f = px.line(dfHist, x='epoch', y='val_loss', title='', markers=True);\n",
        "f = f.update_layout(height=200, margin=dict(l=0, r=0, t=0, b=0))\n",
        "f = f.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "History object contains: loss, accuracy, val_loss, val_accuracy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.6.3.min.js\"></script>                <div id=\"56532015-3862-4016-8c54-ff892179a607\" class=\"plotly-graph-div\" style=\"height:200px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"56532015-3862-4016-8c54-ff892179a607\")) {                    Plotly.newPlot(                        \"56532015-3862-4016-8c54-ff892179a607\",                        [{\"hovertemplate\":\"epoch=%{x}<br>val_loss=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[1,2,3,4,5,6],\"xaxis\":\"x\",\"y\":[0.19082283973693848,0.0803283303976059,0.05107969790697098,0.047065701335668564,0.033796146512031555,0.028375444933772087],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"val_loss\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":0,\"l\":0,\"r\":0,\"b\":0},\"height\":200},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('56532015-3862-4016-8c54-ff892179a607');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvf6AbnRsZU5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "45777eba-3726-4a81-97eb-05eaee346f10"
      },
      "source": [
        "pOneHot = m.predict(vX)   # probabilities for each category. Subjects are rows\n",
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(pOneHot[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col0,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col5,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col5{\n",
              "            background-color:  #b40426;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col1,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col2,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col3,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col4,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col5,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col0,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col1,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col2,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col3,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col4,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col0,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col1,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col2,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col3,#T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col4{\n",
              "            background-color:  #3b4cc0;\n",
              "            color:  #f1f1f1;\n",
              "        }</style><table id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002\" class=\"dataframe\"><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0/walking</th>        <th class=\"col_heading level0 col1\" >1/walking_upstairs</th>        <th class=\"col_heading level0 col2\" >2/walking_downstairs</th>        <th class=\"col_heading level0 col3\" >3/sitting</th>        <th class=\"col_heading level0 col4\" >4/standing</th>        <th class=\"col_heading level0 col5\" >5/laying</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.996505</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.001159</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.002076</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.000001</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0.000257</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row0_col5\" class=\"data row0 col5\" >0.000001</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.000006</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.000140</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row1_col5\" class=\"data row1 col5\" >0.999854</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.000001</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.001125</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "                        <td id=\"T_91bf43d0_545d_11ec_93c4_0242ac1c0002row2_col5\" class=\"data row2 col5\" >0.998874</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f1d0f163d50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHyhHcGyktLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "c888d2fc-658f-41d1-d2df-86dee4b0e811"
      },
      "source": [
        "pY = pd.DataFrame(np.argmax(pOneHot, axis = 1)+1, columns=['y'])  # predicted labels (from 1 to 6)\n",
        "pY.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>...</th>\n",
              "      <th>2897</th>\n",
              "      <th>2898</th>\n",
              "      <th>2899</th>\n",
              "      <th>2900</th>\n",
              "      <th>2901</th>\n",
              "      <th>2902</th>\n",
              "      <th>2903</th>\n",
              "      <th>2904</th>\n",
              "      <th>2905</th>\n",
              "      <th>2906</th>\n",
              "      <th>2907</th>\n",
              "      <th>2908</th>\n",
              "      <th>2909</th>\n",
              "      <th>2910</th>\n",
              "      <th>2911</th>\n",
              "      <th>2912</th>\n",
              "      <th>2913</th>\n",
              "      <th>2914</th>\n",
              "      <th>2915</th>\n",
              "      <th>2916</th>\n",
              "      <th>2917</th>\n",
              "      <th>2918</th>\n",
              "      <th>2919</th>\n",
              "      <th>2920</th>\n",
              "      <th>2921</th>\n",
              "      <th>2922</th>\n",
              "      <th>2923</th>\n",
              "      <th>2924</th>\n",
              "      <th>2925</th>\n",
              "      <th>2926</th>\n",
              "      <th>2927</th>\n",
              "      <th>2928</th>\n",
              "      <th>2929</th>\n",
              "      <th>2930</th>\n",
              "      <th>2931</th>\n",
              "      <th>2932</th>\n",
              "      <th>2933</th>\n",
              "      <th>2934</th>\n",
              "      <th>2935</th>\n",
              "      <th>2936</th>\n",
              "      <th>2937</th>\n",
              "      <th>2938</th>\n",
              "      <th>2939</th>\n",
              "      <th>2940</th>\n",
              "      <th>2941</th>\n",
              "      <th>2942</th>\n",
              "      <th>2943</th>\n",
              "      <th>2944</th>\n",
              "      <th>2945</th>\n",
              "      <th>2946</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows √ó 2947 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38    39    40    41    42    43    44    45    46    47    48    49    ...  2897  2898  2899  2900  2901  2902  2903  2904  2905  2906  2907  2908  2909  2910  2911  2912  2913  2914  2915  2916  2917  2918  2919  2920  2921  2922  2923  2924  2925  2926  2927  2928  2929  2930  2931  2932  2933  2934  2935  2936  2937  2938  2939  2940  2941  2942  2943  2944  2945  2946\n",
              "y     1     6     6     2     1     6     2     6     1     5     6     3     6     5     1     5     3     2     2     2     2     3     4     2     5     1     4     5     5     5     1     3     2     3     3     2     5     5     6     5     3     5     6     6     6     2     2     2     6     3  ...     2     5     4     5     2     3     2     6     1     5     1     5     2     3     4     4     2     2     1     3     1     5     5     2     6     3     6     6     6     3     6     5     3     5     1     4     1     3     6     6     2     1     3     2     4     5     6     3     4     2\n",
              "\n",
              "[1 rows x 2947 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEmcHGfFsobz"
      },
      "source": [
        "ToCSV(pY, 'HAR_baseline')  # generate a CSV submission file for Kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbty_ZankUul"
      },
      "source": [
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4qqqpKjDsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c95ccde-0676-4820-9b2f-860231748734"
      },
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 51 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkiAK0ztkaI_"
      },
      "source": [
        "# üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgh9fVRo-oA6"
      },
      "source": [
        "1. Try tuning DNN hyperparameters\n",
        "1. Training set has 500K observations (2GB), but you really don't need them all. They are all bootstrapped (with noise) from the original sample of 7352 observations. In order to stay within Colab runtime limit (CRTL), you can \n",
        "  1. use more observations for a shallow DNN, but risk underfitting due to lower model complexity\n",
        "  1. use fewer observations for a deeper DNN, but risk overfitting to higher model complexity\n",
        "1. Check out the original related papers about feature engineering for this dataset\n",
        "1. Try engineering features with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and discarding unimportant features via PCA or alternative technique.\n",
        "1. Consider KMeans/KMedoid or other clustering methods to identify observations, which represent the original 7352 observations. It might require finding 7352 cluster centroids/medoids.\n",
        "  1. Fast clustering methods: [FAISS](https://github.com/facebookresearch/faiss) (GPU-enabled)\n",
        "1. For deep NN, consider dropout, batch normalization\n",
        "1. Try PCA on transposed matrix to find/eliminate highly correlated observations\n",
        "1. Try [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to ensure each label is proportionally represented in a subsample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HngvaaXwnVAE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}